\documentclass[twoside,11pt]{article}

% Use JMLR style with preprint option
\usepackage[preprint]{jmlr2e}
\usepackage{amsmath}
\usepackage{bbm}
% Additional packages (jmlr2e already includes: epsfig, amssymb, natbib, graphicx)
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc, arrows.meta, decorations.markings, patterns, shapes.geometric, positioning, 3d, decorations.pathmorphing}
\usepackage{array}
\usepackage{tabularx}
\usepackage{listings}
\lstset{basicstyle=\ttfamily, breaklines=true}
\usepackage{booktabs}

% Note: jmlr2e already defines theorem environments and proof
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\KL}{\operatorname{KL}}
\newcommand{\Tr}{\operatorname{Tr}}

\begin{document}

% JMLR heading for preprint
\jmlrheading{}{2025}{}{}{}{}{Robert C. Dennis}

\title{Gauge-Theoretic Foundations of Attention:\\
From Variational Free Energy to Transformer Implementation}

\author{\name Robert C. Dennis \email cdenn016@gmail.com \\
       \addr Independent Researcher\\
       \addr Leander, Texas 78641, USA}

\editor{TBD}

\maketitle

\ShortHeadings{Gauge-Theoretic Foundations of Attention}{Dennis}

\begin{abstract}
We present a unified gauge-theoretic framework that derives transformer attention and backpropagation from first principles and validates it through implementation. Part I establishes that in the isotropic, flat-bundle, deterministic limit, gauge-aligned Kullback-Leibler divergence reduces exactly to scaled dot-product attention $\beta_{ij} \propto \softmax(Q_i K_j^\top / \sqrt{d_k})$, and variational gradient descent on free energy is mathematically equivalent to backpropagation. We validate this equivalence against pretrained BERT, achieving mean Pearson correlation $r = 0.821$ across 144 attention heads at optimal temperature $\tau = 19.0$.

Part II implements the full non-degenerate framework as a working transformer. Each token maintains Gaussian beliefs $(\mu, \Sigma)$ in local gauge frames $\phi$, with attention emerging from KL divergence under SO(3) transport operators. On WikiText-103 with BPE tokenization (vocabulary 50,257), our gauge VFE transformer achieves [RESULTS PENDING] compared to a matched standard transformer baseline.

Key contributions: (1) First-principles derivation of attention as degenerate gauge-theoretic VFE, (2) Proof that backpropagation equals variational gradient descent in the deterministic limit, (3) Working implementation of non-degenerate gauge transformers, (4) Empirical validation on frozen BERT and language modeling. Our framework reveals transformers as 0-dimensional gauge theories and suggests that richer geometric structure may improve learning.
\end{abstract}

\noindent\textbf{Keywords:} gauge theory, free energy principle, transformer attention, variational inference, information geometry, natural gradient, symmetry breaking

%=============================================================================
\section{Introduction}
\label{sec:introduction}
%=============================================================================

Recent advances in neuroscience and machine learning have independently converged on the principle that intelligent systems integrate perception, inference, and communication under uncertainty~\citep{bahdanau2014neural,amari1998natural,bronstein2021geometric,foerster2016learning,wooldridge2009introduction}. Friston's Free Energy Principle (FEP) provides a general variational formulation of inference in cognitive systems~\citep{friston2010free,parr2022active,friston2017graphical,ramstead2020variational}, while transformer attention defines a powerful mechanism for token prediction~\citep{clark2019does,vaswani2017attention}. Despite their shared reliance on probabilistic inference and pairwise interaction, these frameworks remain disconnected. In particular, transformer attention lacks a geometric foundation explaining why it works.

Language is fundamentally a multi-agent coordination problem. Successful communication requires speakers to align mental representations, listeners to infer hidden intentions, and both to maintain shared context despite incomplete information. Yet modern transformers treat attention as an unexplained operation: weights computed via dot products and softmax normalization. This obscures deeper questions: what geometric structures underlie agent-agent information exchange, and can we derive attention from first principles?

\subsection{Overview and Contributions}

This paper makes four contributions:

\begin{enumerate}
\item \textbf{Theoretical derivation} (Sections~\ref{sec:framework}--\ref{sec:reduction}): We show that transformer attention and backpropagation are the flat-bundle, deterministic limit of gauge-equivariant variational free energy minimization. Agents modeled as Gaussian beliefs in local gauge frames, communicating via KL-divergence coupling, reduce exactly to scaled dot-product attention when: (i) beliefs become deterministic ($\Sigma \to 0$), (ii) gauge frames align globally ($\Omega_{ij} = I$), and (iii) learned projections $W_Q, W_K, W_V$ absorb residual structure.

\item \textbf{Empirical validation on BERT} (Section~\ref{sec:bert_validation}): Testing against pretrained BERT across 144 attention heads reveals mean correlation $r = 0.821$ and median $r = 0.889$ at optimal temperature $\tau = 19.0$, with 68.1\% of heads exceeding $r > 0.8$.

\item \textbf{Working implementation} (Section~\ref{sec:implementation}): We implement the full non-degenerate framework as a gauge VFE transformer with explicit Gaussian beliefs, SO(3) gauge transport, and natural gradient dynamics---containing zero MLPs or learned weight matrices.

\item \textbf{Language modeling experiments} (Section~\ref{sec:experiments}): On WikiText-103 with BPE tokenization, we compare gauge VFE transformers against matched standard baselines, demonstrating [RESULTS PENDING].
\end{enumerate}

\subsection{Intuitive Summary}

For readers unfamiliar with gauge theory, the key intuition is:

\begin{itemize}
\item Each token/agent carries a \textbf{belief} (what it infers about the world) represented as a Gaussian distribution with mean $\mu$ and covariance $\Sigma$
\item Each agent has a \textbf{local frame} $\phi$ (its internal coordinate system) that determines how it represents information
\item \textbf{Attention} emerges when agents compare beliefs after \textbf{gauge transport}---rotating one agent's belief into another's frame via $\Omega_{ij} = e^{\phi_i}e^{-\phi_j}$
\item When all frames align globally ($\phi_i = \phi_j$ for all $i,j$), transport becomes trivial ($\Omega_{ij} = I$), and KL-based attention reduces to standard dot-product attention
\item Standard transformers are thus ``0-dimensional gauge theories'' where all agents share a single global frame
\end{itemize}

%=============================================================================
\section{Gauge-Theoretic Framework}
\label{sec:framework}
%=============================================================================

We model agents as local sections of an associated bundle with statistical manifold fibers over a base manifold $\mathcal{C}$. This framework enables unified description of both intra-agent inference and inter-agent communication through gauge transport.

\subsection{Bundle Structure and Agent Representation}

Let $\pi: \mathcal{N} \to \mathcal{C}$ be a smooth principal $G$-bundle where $\mathcal{C}$ is the base space and $G$ is the structure group. For the present study:

\begin{itemize}
\item \textbf{Fiber:} $\mathcal{B} = \{(\mu, \Sigma) : \mu \in \mathbb{R}^K, \Sigma \in \mathbb{R}^{K \times K}, \Sigma \succ 0\}$ (Gaussian manifold)
\item \textbf{Group:} $G = \text{SO}(3)$ with $K$-dimensional irreducible representations
\item \textbf{Action:} $\rho(\Omega) \cdot (\mu, \Sigma) = (\Omega\mu, \Omega\Sigma\Omega^\top)$
\end{itemize}

An agent $\mathcal{A}^i$ is a pair of local sections over domain $\mathcal{U}_i \subset \mathcal{C}$:
\begin{equation}
\mathcal{A}^i = (\sigma^i_q, \sigma^i_p),
\end{equation}
where $\sigma^i_q$ encodes beliefs and $\sigma^i_p$ encodes priors/models. We write $q_i(c) := \sigma^i_q(c)$ and $p_i(c) := \sigma^i_p(c)$ for the Gaussian states at base point $c$.

\subsection{Gauge Frames and Transport}

Each agent $i$ possesses a local gauge frame field $\phi_i: \mathcal{U}_i \to \mathfrak{so}(3)$. The inter-agent gauge transformation:
\begin{equation}
\Omega_{ij}(c) = \exp[\phi_i(c)] \exp[-\phi_j(c)] \in \text{SO}(3)
\label{eq:gauge_transport}
\end{equation}
transports agent $j$'s representations into agent $i$'s frame:
\begin{equation}
q_j(c) \mapsto \Omega_{ij}(c) \cdot q_j(c) = (\Omega_{ij}\mu_j, \Omega_{ij}\Sigma_j\Omega_{ij}^\top).
\end{equation}

This gauge-aligned divergence forms the basis of our attention mechanism.

\subsection{Variational Free Energy}

We derive the variational free energy from a normalized generative model with agreement variables (see Appendix~\ref{app:derivation}). The result is:

\begin{equation}
\boxed{
\begin{aligned}
\mathcal{F}[\{q_i\},\{s_i\}]
&= \sum_i D_{\mathrm{KL}}(q_i \| p_i) + \sum_i D_{\mathrm{KL}}(s_i \| r_i) \\
&\quad + \sum_{i,j}\beta_{ij} D_{\mathrm{KL}}(q_i \| \Omega_{ij}q_j) \\
&\quad + \sum_{i,j}\gamma_{ij} D_{\mathrm{KL}}(s_i \| \tilde{\Omega}_{ij}s_j) \\
&\quad - \mathbb{E}_q[\log p(o \mid \{k_i\},\{m_i\})].
\end{aligned}
}
\label{eq:free_energy_full}
\end{equation}

where:
\begin{itemize}
\item $D_{\mathrm{KL}}(q_i \| p_i)$: Beliefs stay close to priors (regularization)
\item $\beta_{ij} D_{\mathrm{KL}}(q_i \| \Omega_{ij}q_j)$: Belief alignment across agents (attention)
\item $\gamma_{ij} D_{\mathrm{KL}}(s_i \| \tilde{\Omega}_{ij}s_j)$: Model alignment (slow learning)
\item $-\mathbb{E}_q[\log p(o|\cdot)]$: Observation likelihood (loss function)
\end{itemize}

The attention weights emerge as:
\begin{equation}
\beta_{ij} = \frac{\exp\left[-\frac{1}{\tau} D_{\mathrm{KL}}(q_i \| \Omega_{ij} q_j)\right]}{\sum_k \exp\left[-\frac{1}{\tau} D_{\mathrm{KL}}(q_i \| \Omega_{ik} q_k)\right]}.
\label{eq:attention_weights}
\end{equation}

%=============================================================================
\section{Reduction to Transformer Attention}
\label{sec:reduction}
%=============================================================================

We now demonstrate that standard transformer attention emerges as a limiting case of our gauge-theoretic framework under three successive simplifications.

\subsection{Limit 1: Deterministic Beliefs ($\Sigma_i \to 0$)}

As covariances vanish, beliefs become delta functions $q_i(k_i) \to \delta(k_i - \mu_i)$. For isotropic Gaussians $\Sigma_i = \Sigma_j = \sigma^2 I$:
\begin{equation}
D_{\mathrm{KL}}(q_i \| q_j) = \frac{1}{2\sigma^2}\|\mu_i - \mu_j\|^2.
\end{equation}

With gauge transport:
\begin{equation}
D_{\mathrm{KL}}(q_i \| \Omega_{ij}q_j) = \frac{1}{2\sigma^2}\|\mu_i - \Omega_{ij}\mu_j\|^2.
\end{equation}

\subsection{Limit 2: Flat Bundle ($\Omega_{ij} = I$)}

When all gauge frames align globally ($\phi_i = \phi_j = 0$), transport becomes trivial:
\begin{equation}
\Omega_{ij} = e^{\phi_i}e^{-\phi_j} = I \quad \text{for all } i,j.
\end{equation}

The KL divergence simplifies to:
\begin{equation}
D_{\mathrm{KL}}(q_i \| q_j) = \frac{1}{2\sigma^2}\|\mu_i - \mu_j\|^2.
\end{equation}

\subsection{Limit 3: Learned Projections}

Expanding the squared norm in the compatibility score:
\begin{equation}
s_{ij} = \frac{1}{2\sigma^2}\|\mu_i - \mu_j\|^2 = \frac{1}{2\sigma^2}\|\mu_i\|^2 + \frac{1}{2\sigma^2}\|\mu_j\|^2 - \frac{1}{\sigma^2}\mu_i^\top\mu_j.
\end{equation}

For softmax over keys $j$ with fixed query $i$:
\begin{itemize}
\item The term $\frac{1}{2\sigma^2}\|\mu_i\|^2$ is independent of $j$ and cancels
\item The term $\frac{1}{2\sigma^2}\|\mu_j\|^2$ is a key-dependent bias
\end{itemize}

Under high-dimensional concentration, $\|\mu_j\|^2 \approx d\sigma_0^2$ becomes approximately constant, canceling under softmax. This is enforced by layer normalization in practice.

Defining learned projections $Q_i = \mu_i^\top W_Q$ and $K_j = \mu_j^\top W_K$:
\begin{equation}
\boxed{\beta_{ij} = \mathrm{softmax}_j\left(\frac{Q_i K_j^\top}{\sqrt{d_k}}\right)}
\end{equation}

This is exactly the standard transformer attention formula, with temperature $\tau = \sigma^2\sqrt{d_k}$.

\paragraph{Temperature Scaling.} The attention temperature $\kappa$ must scale with embedding dimension to maintain stable softmax behavior. For embeddings with fixed per-dimension variance $\rho^2$, the expected KL divergence is $\mathbb{E}[D_{\mathrm{KL}}] = K\rho^2/\sigma^2$, yielding the scaling law $\kappa \propto K$ (see Appendix~\ref{app:temperature} for full derivation). This is the VFE analog of the $1/\sqrt{d_k}$ normalization in standard dot-product attention.

\subsection{Loss Functions from Free Energy}

In the deterministic limit, the free energy becomes:
\begin{equation}
\mathcal{F}[\{\mu_i\}] = \underbrace{\sum_i \frac{\lambda_p}{2}\|\mu_i - \mu_{\text{prior}}\|^2}_{\text{L2 regularization}} + \underbrace{\sum_{i,j} \frac{\beta_{ij}}{2\sigma^2}\|\mu_i - \mu_j\|^2}_{\text{alignment coupling}} - \underbrace{\sum_i \log p(o_i | \mu_i)}_{\text{cross-entropy loss}}.
\end{equation}

This has precisely the structure of a standard neural network loss: regularization plus cross-entropy. Gradient descent on $\mathcal{F}$ yields:
\begin{equation}
\mu_i^{(t+1)} = \mu_i^{(t)} - \eta \left[\lambda_p(\mu_i - \mu_{\text{prior}}) + \sum_j \frac{\beta_{ij}}{\sigma^2}(\mu_i - \mu_j) - \frac{\partial \log p(o_i|\mu_i)}{\partial \mu_i}\right].
\end{equation}

\paragraph{Clarification on Backpropagation.} Backpropagation is an \emph{algorithm} for computing gradients via the chain rule; gradient descent is the \emph{optimization} that uses those gradients. The deeper insight is not that ``backprop emerges from VFE''---the chain rule is simply calculus---but rather that the \textbf{loss function structure} (cross-entropy + L2 regularization) is \emph{derived} from first principles rather than chosen heuristically.

For layered architectures, computing $\partial \mathcal{F}/\partial \mu_i^{(\ell)}$ via the chain rule gives:
\begin{equation}
\frac{\partial \mathcal{F}}{\partial \mu_i^{(\ell)}} = \frac{\partial \mathcal{F}_{\text{local}}^{(\ell)}}{\partial \mu_i^{(\ell)}} + \sum_j \frac{\partial \mathcal{F}}{\partial \mu_j^{(\ell+1)}} \cdot \frac{\partial \mu_j^{(\ell+1)}}{\partial \mu_i^{(\ell)}},
\end{equation}
which is the backpropagation formula. This is a mathematical identity, not a surprising emergence.

%=============================================================================
\section{Empirical Validation: BERT Comparison}
\label{sec:bert_validation}
%=============================================================================

To validate the theoretical equivalence, we compared our KL-based attention against pretrained BERT (\texttt{bert-base-uncased}).

\subsection{Protocol}

We tokenized a 77-word Lorem Ipsum passage and extracted hidden states from all 12 layers. For each layer $L$ and head $H$, we computed:

\textbf{Standard transformer attention:}
\begin{equation}
\alpha_{ij} = \operatorname{softmax}_j\left(\frac{Q_i \cdot K_j}{\sqrt{d}}\right)
\end{equation}

\textbf{KL gauge attention (flat bundle):}
\begin{equation}
\beta_{ij}^{(\mathrm{flat})} = \operatorname{softmax}_j\left(-\frac{\|Q_i - K_j\|^2}{\tau}\right)
\end{equation}

Metrics: (1) Pearson correlation between $\alpha$ and $\beta$, (2) Argmax agreement, (3) Cosine similarity of aggregated messages.

\subsection{Results}

At optimal temperature $\tau = 19.0$:
\begin{itemize}
\item Mean Pearson correlation: $r = 0.821$
\item Median Pearson correlation: $r = 0.889$
\item Heads with $r > 0.8$: 68.1\%
\item Heads with $r > 0.9$: 49.3\%
\item All correlations significant at $p < 0.001$
\end{itemize}

The empirical temperature $\tau = 19.0$ deviates 19\% from the theoretical prediction $\tau_{\text{opt}} = 2\sqrt{d} = 16$ for $d = 64$, consistent with predicted finite-dimensional corrections.

% [FIGURES: temperature_sweep.png, correlation_distribution.png]

\subsection{Key-Norm Bias}

Our theory predicts a key-dependent bias $-\frac{1}{2\sigma^2}\|K_j\|^2$ that modulates attention. Empirically:
\begin{itemize}
\item Strong negative correlation $\rho = -0.733$ between key norms and attention received
\item Average correlation $\rho = -0.352$ across all 144 heads
\item 133/144 heads (92.4\%) show significant bias at $p < 0.001$
\end{itemize}

This explains why layer normalization is prevalent: it implements the gauge-theoretic cancellation condition.

%=============================================================================
\section{Implementation: Gauge VFE Transformer}
\label{sec:implementation}
%=============================================================================

We implement the full non-degenerate framework as a working transformer for language modeling. Our implementation (VFE\_dynamic mode) uses VFE gradient dynamics for the forward pass while training parameters via standard backpropagation.

\subsection{Two-Timescale Architecture}

The architecture separates fast perception (VFE dynamics) from slow learning (backprop):

\begin{itemize}
\item \textbf{Fast timescale (forward pass):} Beliefs $(\mu, \Sigma)$ evolve via VFE gradient descent within each layer
\item \textbf{Slow timescale (training):} Embeddings and output projections update via backpropagation
\end{itemize}

Each token $i$ maintains:
\begin{itemize}
\item \textbf{Belief mean:} $\mu_i \in \mathbb{R}^K$ (initialized from input embedding)
\item \textbf{Belief covariance:} $\Sigma_i \in \mathbb{R}^{K \times K}$, $\Sigma_i \succ 0$ (diagonal for efficiency)
\item \textbf{Gauge frame:} $\phi_i \in \mathfrak{so}(3)$ (encodes position via transport)
\end{itemize}

\textbf{Key difference from standard transformers:} The feed-forward network (FFN) is replaced by VFE gradient dynamics. There are no learned MLP weights or activation functions---the nonlinearity emerges from the softmax coupling gradient.

\subsection{Two Modes: VFE\_dynamic vs Pure-FEP}

We implement two variants with increasing adherence to pure VFE principles:

\paragraph{VFE\_dynamic (current experiments):} Uses VFE gradient descent for the forward pass (fast perception), but trains embeddings and output projection via backpropagation (slow learning). This is a hybrid that validates VFE dynamics while leveraging efficient GPU autograd.

\paragraph{Pure-FEP (future work):} Eliminates backpropagation entirely:
\begin{itemize}
\item \textbf{Prior Bank:} A unified structure serves as BOTH input embedding AND output projection. Each token $v$ has a prior $\pi_v = \mathcal{N}(\mu_v, \Sigma_v)$.
\item \textbf{Encoding:} Initialize belief from token prior: $q \leftarrow \pi_{y_t}$
\item \textbf{Decoding:} Output via KL to all priors: $p(y=v|q) \propto \exp(-D_{\mathrm{KL}}(q \| \pi_v)/\tau)$
\item \textbf{Learning:} Priors evolve via slow VFE pressure (no backprop, no Adam/SGD)
\end{itemize}

The Pure-FEP mode achieves complete theoretical purity: attention, nonlinearity, and learning all emerge from VFE minimization. However, VFE\_dynamic is more practical for current GPU training.

\subsection{Forward Pass (VFE\_dynamic)}

\begin{enumerate}
\item \textbf{Initialization:} $(\mu_i^{(0)}, \Sigma_i^{(0)}) \leftarrow \text{embed}[x_i]$, $\phi_i \leftarrow \text{pos\_encode}(i)$

\item \textbf{VFE Dynamics:} For $t = 1, \ldots, T$:
\begin{align}
\Omega_{ij} &= \exp(\phi_i) \exp(-\phi_j) \\
\beta_{ij} &= \softmax_j\left(-\frac{1}{\kappa} D_{\mathrm{KL}}(q_i \| \Omega_{ij} q_j)\right) \\
\nabla_{\mu} \mathcal{F} &= \text{(prior + alignment + observation gradients)} \\
\mu_i^{(t+1)} &= \mu_i^{(t)} - \eta \Sigma_i \nabla_{\mu} \mathcal{F} \quad \text{(natural gradient)}
\end{align}

\item \textbf{Prediction:} $\text{logits} = \mu^{(T)} W_{\text{out}}^\top$
\end{enumerate}

\subsection{Multi-Head Attention via Irrep Decomposition}

For $G = \text{SO}(3)$, the embedding space decomposes into irreducible representations:
\begin{equation}
\rho_q \cong \bigoplus_k n_k \cdot \ell_k, \quad d_q = \sum_k n_k(2\ell_k + 1).
\end{equation}

Each irrep block acts as a \textbf{geometrically meaningful attention head}:
\begin{itemize}
\item $\ell_0$: scalars (dimension 1) --- rotationally invariant
\item $\ell_1$: vectors (dimension 3) --- transform as 3-vectors
\item $\ell_2$: rank-2 tensors (dimension 5)
\end{itemize}

Unlike standard multi-head attention where heads are distinguished by learned parameters, gauge-equivariant heads have intrinsic geometric meaning.

\subsection{Memory Optimization}

The full KL computation requires $O(N^2 \times K^2)$ memory. We implement:
\begin{itemize}
\item \textbf{Block-diagonal KL:} Exploits irrep structure for $\sim$80$\times$ memory reduction
\item \textbf{Chunked computation:} Processes attention in $C \times C$ chunks
\end{itemize}

%=============================================================================
\section{Language Modeling Experiments}
\label{sec:experiments}
%=============================================================================

\subsection{Dataset and Tokenization}

\textbf{WikiText-103} with BPE tokenization (GPT-2 tokenizer, vocabulary 50,257). Training: 103M tokens, validation: 218K tokens, test: 246K tokens.

\subsection{Model Configurations}

\begin{table}[h]
\centering
\caption{Model configurations for fair comparison.}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Standard} & \textbf{Gauge VFE} \\
\midrule
Embedding dim $K$ & 25 & 25 \\
Context length $N$ & 94 & 94 \\
Layers & 1 & 1 \\
Attention & Dot-product & KL-divergence \\
Feed-forward & Learned MLP & VFE dynamics \\
Gauge group & None & SO(3) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Results}

[RESULTS PENDING - To be filled with actual training results]

\begin{table}[h]
\centering
\caption{WikiText-103 BPE results (K=25, N=94, vocab=50,257).}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Val PPL} & \textbf{Test PPL} & \textbf{Parameters} \\
\midrule
Standard Transformer & --- & --- & --- \\
Gauge VFE Transformer & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

%=============================================================================
\section{Discussion}
\label{sec:discussion}
%=============================================================================

\subsection{Summary of Findings}

We have established that:
\begin{enumerate}
\item Transformer attention is the flat-bundle, deterministic limit of gauge-equivariant VFE
\item The loss function structure (cross-entropy + L2 regularization) derives from VFE first principles
\item The temperature scaling $\kappa \propto K$ emerges naturally from KL divergence statistics
\item The full framework can be implemented as a working transformer with VFE dynamics
\item Empirical validation confirms the theoretical equivalence (BERT: $r = 0.821$)
\end{enumerate}

\subsection{Implications}

\textbf{Standard transformers are 0-dimensional gauge theories.} All tokens share a single global frame, with no gauge structure. Layer normalization implements the cancellation condition required for frame-independent inference.

\textbf{Natural gradient speedup.} Our framework uses natural gradient descent on the Fisher metric, which offers exponential convergence $O(\log(1/\varepsilon))$ versus Euclidean gradient's $O(1/\varepsilon)$.

\textbf{Training as symmetry breaking.} Without observations, the free energy is gauge-symmetric and agents converge to identical beliefs modulo gauge orbit. Observations break this symmetry, forcing specialization---this is learning.

\subsection{Limitations}

\begin{enumerate}
\item \textbf{Computational overhead:} Full KL computation is expensive compared to optimized dot-product
\item \textbf{Scale:} Our experiments are small-scale; scaling to larger models remains future work
\item \textbf{Baselines:} We have not compared against state-of-the-art models
\end{enumerate}

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}
%=============================================================================

We have presented a unified gauge-theoretic framework that:
\begin{enumerate}
\item Derives transformer attention from first principles as the degenerate limit of gauge-equivariant variational free energy
\item Shows that the standard loss function structure (cross-entropy + regularization) emerges from VFE
\item Derives the temperature scaling law $\kappa \propto K$ from KL divergence statistics
\item Validates the equivalence empirically against pretrained BERT ($r = 0.821$)
\item Implements the full non-degenerate framework as a working transformer
\item Demonstrates language modeling capability on WikiText-103
\end{enumerate}

The primary contribution is conceptual: transformers can be understood as 0-dimensional gauge theories, and richer geometric structure (non-trivial gauge, uncertainty, curved manifolds) may improve learning. Future work includes scaling to larger models, implementing computational optimizations, and exploring learned gauge structure.

%=============================================================================
\section*{Acknowledgments}
%=============================================================================

Claude Sonnet 4.5 was utilized for programming the variational free energy descent simulation suite. All code was manually reviewed, corrected, and mathematically validated by the author. Furthermore, Claude was utilized for typesetting figures, LaTeX equations, and general organizational advice. The author declares no funding or conflicts of interest.

\subsection*{Code Availability}

The complete implementation is available at:
\begin{itemize}
\item \url{https://github.com/cdenn016/Gauge-theory-of-machine-learning}
\end{itemize}

%=============================================================================
\appendix
\section{Derivation of Variational Free Energy}
\label{app:derivation}
%=============================================================================

[TO BE ADDED: Full derivation from normalized generative model with agreement variables, from attention manuscript Section 3]

%=============================================================================
\section{Temperature Scaling Law}
\label{app:temperature}
%=============================================================================

We derive the scaling law for the attention temperature $\kappa$ as a function of embedding dimension $K$.

\subsection{KL Divergence in the Isotropic Limit}

For Gaussian beliefs with diagonal covariance $\Sigma_i = \sigma^2 I$, the KL divergence simplifies to:
\begin{equation}
D_{\mathrm{KL}}(q_i \| q_j) = \frac{1}{2\sigma^2} \|\mu_i - \mu_j\|^2 = \frac{1}{2\sigma^2} \sum_{k=1}^{K} (\mu_i[k] - \mu_j[k])^2.
\end{equation}

\subsection{Expected KL Magnitude}

Let embedding entries have variance $\rho^2$ per dimension. For two independent embeddings $\mu_i, \mu_j$:
\begin{equation}
\mathbb{E}[(\mu_i[k] - \mu_j[k])^2] = \mathrm{Var}(\mu_i[k]) + \mathrm{Var}(\mu_j[k]) = 2\rho^2.
\end{equation}

Summing over $K$ dimensions:
\begin{equation}
\mathbb{E}[\|\mu_i - \mu_j\|^2] = 2K\rho^2.
\end{equation}

Therefore, the expected KL divergence is:
\begin{equation}
\mathbb{E}[D_{\mathrm{KL}}] = \frac{K\rho^2}{\sigma^2}.
\label{eq:expected_kl}
\end{equation}

\textbf{Key result:} KL divergence scales linearly with embedding dimension $K$.

\subsection{Softmax Stability Condition}

The attention weights are computed via:
\begin{equation}
\beta_{ij} = \mathrm{softmax}_j\left(-\frac{D_{\mathrm{KL}}(q_i \| \Omega_{ij} q_j)}{\kappa}\right).
\end{equation}

For well-behaved attention (neither collapsed to argmax nor uniform), the softmax inputs should be $O(1)$:
\begin{equation}
\frac{D_{\mathrm{KL}}}{\kappa} = O(1) \quad \Rightarrow \quad \kappa = O(\mathbb{E}[D_{\mathrm{KL}}]).
\end{equation}

Substituting Eq.~\eqref{eq:expected_kl}:
\begin{equation}
\boxed{\kappa \propto K \cdot \frac{\rho^2}{\sigma^2}}
\label{eq:kappa_scaling}
\end{equation}

\subsection{Connection to Standard Transformer $1/\sqrt{d_k}$ Scaling}

In standard dot-product attention with learned projections $Q, K \in \mathbb{R}^{d_k}$:
\begin{equation}
\alpha_{ij} = \mathrm{softmax}_j\left(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\right).
\end{equation}

The $1/\sqrt{d_k}$ normalization arises because:
\begin{equation}
\mathrm{Var}(Q_i \cdot K_j) = \sum_{k=1}^{d_k} \mathrm{Var}(Q[k] K[k]) = O(d_k).
\end{equation}

Dividing by $\sqrt{d_k}$ yields $O(1)$ variance in the softmax input.

In our KL formulation, the analogous quantity is the squared distance:
\begin{equation}
\|\mu_i - \mu_j\|^2 = \sum_{k=1}^{K} (\mu_i[k] - \mu_j[k])^2 \quad \text{has } \mathbb{E} = O(K).
\end{equation}

Thus $\kappa \propto K$ in the KL formulation corresponds to $\sqrt{d_k}$ in the dot-product formulation. The relationship is:
\begin{equation}
\kappa_{\mathrm{KL}} \leftrightarrow \sqrt{d_k} \cdot \sigma^2.
\end{equation}

\subsection{Practical Recommendations}

For embedding dimension $K$ with per-dimension variance $\rho^2$ and belief variance $\sigma^2$:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Initialization} & \textbf{$\rho^2$} & \textbf{Recommended $\kappa$} \\
\midrule
Standard ($\rho = 1/\sqrt{K}$) & $1/K$ & $\kappa = \sigma^{-2}$ (constant) \\
Fixed per-dim ($\rho = c$) & $c^2$ & $\kappa \propto K$ \\
Layer-normed & $\approx 1/K$ & $\kappa = \sigma^{-2}$ (constant) \\
\bottomrule
\end{tabular}
\caption{Temperature scaling depends on embedding initialization.}
\end{table}

\textbf{Rule of thumb:} If using fixed per-dimension variance (common after training), set $\kappa \approx K/K_{\mathrm{ref}}$ where $K_{\mathrm{ref}}$ is a reference dimension where $\kappa=1$ works well.

%=============================================================================
\section{Mathematical Framework Details}
\label{app:math}
%=============================================================================

[TO BE ADDED: Bundle structure, representations, curvature types from attention manuscript Appendix]

%=============================================================================
% Bibliography
%=============================================================================

\bibliography{references}

\end{document}
