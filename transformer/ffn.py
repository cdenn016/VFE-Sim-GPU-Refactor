"""
Feed-Forward Networks for Gauge Transformer
===========================================

Supports FIVE modes:
1. LEARNED: Standard FFN with learned weights (default)
2. VARIATIONAL_APPROX: Approximate variational descent (legacy, μ only, no ∂β/∂μ)
3. VARIATIONAL_FULL: Full variational descent (legacy, μ only, with ∂β/∂μ)
4. VARIATIONAL_GRADIENT_ENGINE: Full active inference via validated gradient_engine.py
   - Updates both μ AND Σ
   - Natural gradients via Fisher-Rao metric
   - All energy terms (self-coupling, alignment, observations, softmax coupling)
   - Theoretically principled and validated!
5. HAMILTONIAN: Symplectic Hamiltonian dynamics on belief space (NEW!)
   - Energy-conserving dynamics via leapfrog integration
   - Full faithful SPD geometry with curvature corrections
   - Phase space: (μ, Σ, φ, π_μ, π_Σ, π_φ)
   - H = T + V where V is the free energy functional

Author: Extended architecture with gradient_engine and Hamiltonian integration
Date: December 2025
"""

import torch
import torch.nn as nn
from typing import Optional, Literal, Tuple, Union

from transformer.variational_ffn import (
    VariationalFFNApproximate,
    VariationalFFNFull,
    VariationalFFNGradientEngine,
    VariationalFFNDynamic,  # Dynamic-β VFE with attention-belief co-evolution
    VariationalFFNDynamicStable,  # Stabilized version with temp annealing + gradient balancing
)
from transformer.hamiltonian_ffn import HamiltonianFFN, MassConfig


class GaugeFFN(nn.Module):
    """
    Unified FFN module supporting learned, variational, and Hamiltonian modes.

    Modes:
        'learned': Standard MLP (default)
        'variational_approx': Approximate variational descent (legacy)
        'variational_full': Full variational descent (legacy)
        'variational_gradient_engine': Full active inference (fixed β)
        'VFE_dynamic': Dynamic-β VFE with attention-belief co-evolution
        'VFE_dynamic_stable': Stabilized dynamic-β with temperature annealing (RECOMMENDED!)
        'hamiltonian': Symplectic Hamiltonian dynamics

    VFE_dynamic_stable is the recommended implementation - it recomputes β at each step
    but includes stabilization to prevent training collapse:
    - Temperature annealing (soft → sharp β)
    - Fresh observation gradients (not frozen)
    - Gradient norm balancing

    Switch via mode parameter or at runtime.
    """

    def __init__(
        self,
        embed_dim: int,
        hidden_dim: int,
        generators: Optional[torch.Tensor] = None,  # (3, K, K)
        dropout: float = 0.1,
        mode: Literal['learned', 'standard', 'variational_approx', 'variational_full', 'variational_gradient_engine', 'VFE', 'VFE_dynamic', 'VFE_dynamic_stable', 'hamiltonian'] = 'learned',
        # Dynamic VFE specific parameters
        vfe_dynamic_m_step_interval: int = 0,  # M-step every N steps (0 = disabled)
        vfe_dynamic_m_step_rate: float = 0.01,  # Prior update rate
        # AD-HOC stabilization (default OFF for first-principles)
        vfe_kappa_start: float = None,       # AD-HOC: Temp annealing (None = use kappa, no annealing)
        vfe_balance_gradients: bool = False, # AD-HOC: Balance gradient norms
        vfe_obs_grad_weight: float = 1.0,    # Relative weight of observation gradient
        vfe_entropy_penalty: float = 0.0,    # AD-HOC: Penalty for uniform β
        vfe_self_attn_damping: float = 0.0,  # AD-HOC: Reduce self-attention (0-1)
        vfe_grad_clip: float = 1e3,          # Numerical stability (overflow prevention)
        # Variational parameters
        alpha: float = 0.001,
        tau_eff: float = 1.0,
        kappa: float = 1.0,
        n_iterations: int = 1,
        learnable_lr: bool = True,
        # Gradient engine specific
        lambda_belief: float = 1.0,
        lambda_prior: float = 0.0,
        lambda_phi: float = 0.0,
        update_sigma: bool = True,
        # Hamiltonian specific
        hamiltonian_dt: float = 0.01,
        hamiltonian_n_steps: int = 10,
        hamiltonian_update_phi: bool = False,
        hamiltonian_momentum_scale: float = 1.0,
        hamiltonian_gamma: float = 0.0,  # Damping (0 = pure Hamiltonian)
        # Hamiltonian mass configuration (from Inertia of Belief paper)
        hamiltonian_mass_use_prior: bool = True,        # Λ_p term
        hamiltonian_mass_use_observation: bool = False,  # Λ_o term
        hamiltonian_mass_use_incoming_social: bool = False,  # Σβ_{ik}Λ̃_{qk} term
        hamiltonian_mass_use_outgoing_recoil: bool = False,  # Σβ_{ji}Λ_{qi} term
        hamiltonian_evolve_mass: bool = False,  # Recompute M at each leapfrog step?
        # Diagonal covariance mode
        diagonal_covariance: bool = False,
    ):
        """
        Initialize unified FFN.

        Args:
            embed_dim: K
            hidden_dim: Hidden layer size (for learned mode)
            generators: SO(3) generators (required for variational/hamiltonian modes)
            dropout: Dropout rate (for learned mode)
            mode: FFN mode:
                - 'learned' or 'standard': Standard MLP (default)
                - 'variational_approx': Approximate variational descent (legacy)
                - 'variational_full': Full variational descent (legacy)
                - 'variational_gradient_engine' or 'VFE': Full active inference
                - 'hamiltonian': Symplectic Hamiltonian dynamics
            alpha: Prior weight (variational/hamiltonian)
            tau_eff: Temperature (variational approx/full)
            kappa: Softmax temperature (variational_full/hamiltonian)
            n_iterations: Inference steps (variational)
            learnable_lr: Learn step size? (variational)
            lambda_belief: Belief alignment weight (gradient_engine/hamiltonian)
            lambda_prior: Prior alignment weight (gradient_engine)
            lambda_phi: Gauge field weight (gradient_engine)
            update_sigma: Update covariances? (gradient_engine/hamiltonian)
            hamiltonian_dt: Time step for leapfrog integration
            hamiltonian_n_steps: Number of leapfrog steps per forward pass
            hamiltonian_update_phi: Evolve gauge field in Hamiltonian dynamics?
            hamiltonian_momentum_scale: Scale for initial momentum sampling
            hamiltonian_gamma: Damping coefficient (0 = pure Hamiltonian, >0 = Langevin-like)
            hamiltonian_mass_use_prior: Include prior precision Λ_p in mass (Inertia of Belief paper)
            hamiltonian_mass_use_observation: Include observation precision Λ_o in mass
            hamiltonian_mass_use_incoming_social: Include incoming social precision in mass
            hamiltonian_mass_use_outgoing_recoil: Include outgoing recoil precision in mass
            hamiltonian_evolve_mass: If True, recompute mass M at each leapfrog step.
                                    Theoretically correct since M depends on Σ, but slower.
                                    If False (default), M computed once and held fixed.
        """
        super().__init__()

        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim

        # =================================================================
        # Normalize mode names for clean three-mode architecture:
        #   - 'standard' -> 'learned' (baseline MLP)
        #   - 'VFE' -> 'variational_gradient_engine' (variational free energy)
        # =================================================================
        if mode == 'standard':
            mode = 'learned'
        elif mode == 'VFE':
            mode = 'variational_gradient_engine'
        self.mode = mode

        # =================================================================
        # Learned FFN (standard transformer)
        # =================================================================
        self.learned_ffn = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, embed_dim),
            nn.Dropout(dropout),
        )

        # =================================================================
        # Variational FFNs (active inference)
        # =================================================================
        if mode in ['variational_approx', 'variational_full', 'variational_gradient_engine', 'VFE_dynamic', 'VFE_dynamic_stable']:
            if generators is None:
                raise ValueError("generators required for variational modes")

            if mode == 'variational_approx':
                self.variational_ffn = VariationalFFNApproximate(
                    embed_dim=embed_dim,
                    generators=generators,
                    alpha=alpha,
                    tau_eff=tau_eff,
                    n_iterations=n_iterations,
                    learnable_lr=learnable_lr,
                )
            elif mode == 'variational_full':
                self.variational_ffn = VariationalFFNFull(
                    embed_dim=embed_dim,
                    generators=generators,
                    alpha=alpha,
                    tau_eff=tau_eff,
                    kappa=kappa,
                    n_iterations=n_iterations,
                    learnable_lr=learnable_lr,
                )
            elif mode == 'VFE_dynamic':
                # Dynamic-β VFE with attention-belief co-evolution
                self.variational_ffn = VariationalFFNDynamic(
                    embed_dim=embed_dim,
                    generators=generators,
                    alpha=alpha,
                    lambda_belief=lambda_belief,
                    kappa=kappa,
                    n_iterations=n_iterations,
                    learnable_lr=learnable_lr,
                    update_sigma=update_sigma,
                    m_step_interval=vfe_dynamic_m_step_interval,
                    m_step_rate=vfe_dynamic_m_step_rate,
                    diagonal_covariance=diagonal_covariance,
                )
            elif mode == 'VFE_dynamic_stable':
                # Stabilized dynamic-β VFE with temperature annealing (RECOMMENDED!)
                self.variational_ffn = VariationalFFNDynamicStable(
                    embed_dim=embed_dim,
                    generators=generators,
                    alpha=alpha,
                    lambda_belief=lambda_belief,
                    kappa=kappa,
                    kappa_start=vfe_kappa_start,
                    n_iterations=n_iterations,
                    learnable_lr=learnable_lr,
                    update_sigma=update_sigma,
                    m_step_interval=vfe_dynamic_m_step_interval,
                    m_step_rate=vfe_dynamic_m_step_rate,
                    diagonal_covariance=diagonal_covariance,
                    # Stabilization parameters
                    balance_gradients=vfe_balance_gradients,
                    obs_grad_weight=vfe_obs_grad_weight,
                    entropy_penalty=vfe_entropy_penalty,
                    self_attn_damping=vfe_self_attn_damping,
                    grad_clip=vfe_grad_clip,
                )
            else:  # variational_gradient_engine
                self.variational_ffn = VariationalFFNGradientEngine(
                    embed_dim=embed_dim,
                    generators=generators,
                    alpha=alpha,
                    lambda_belief=lambda_belief,
                    lambda_prior=lambda_prior,
                    lambda_phi=lambda_phi,
                    kappa_beta=kappa,
                    n_iterations=n_iterations,
                    learnable_lr=learnable_lr,
                    update_sigma=update_sigma,
                )

        # =================================================================
        # Hamiltonian FFN (symplectic dynamics)
        # =================================================================
        if mode == 'hamiltonian':
            if generators is None:
                raise ValueError("generators required for hamiltonian mode")

            # Create MassConfig from Inertia of Belief paper parameters
            mass_config = MassConfig(
                use_prior_precision=hamiltonian_mass_use_prior,
                use_observation_precision=hamiltonian_mass_use_observation,
                use_incoming_social=hamiltonian_mass_use_incoming_social,
                use_outgoing_recoil=hamiltonian_mass_use_outgoing_recoil,
            )

            self.hamiltonian_ffn = HamiltonianFFN(
                embed_dim=embed_dim,
                generators=generators,
                n_leapfrog_steps=hamiltonian_n_steps,
                dt=hamiltonian_dt,
                alpha=alpha,
                lambda_belief=lambda_belief,
                kappa=kappa,
                update_Sigma=update_sigma,
                update_phi=hamiltonian_update_phi,
                momentum_scale=hamiltonian_momentum_scale,
                mass_config=mass_config,  # Extended mass from paper
                gamma=hamiltonian_gamma,
                evolve_mass=hamiltonian_evolve_mass,  # Recompute M at each step?
                diagonal_covariance=diagonal_covariance,
            )

    def forward(
        self,
        mu: torch.Tensor,          # (B, N, K) - always required
        # Variational/Hamiltonian inputs (optional for learned mode)
        beta: Optional[torch.Tensor] = None,      # (B, n_heads, N, N) or (B, N, N)
        mu_prior: Optional[torch.Tensor] = None,  # (B, N, K)
        phi: Optional[torch.Tensor] = None,       # (B, N, 3)
        sigma: Optional[torch.Tensor] = None,     # (B, N, K, K)
        sigma_prior: Optional[torch.Tensor] = None,  # (B, N, K, K) - for Hamiltonian
        mask: Optional[torch.Tensor] = None,      # (B, N, N)
        # Observation inputs (for gradient_engine/hamiltonian E-step)
        targets: Optional[torch.Tensor] = None,   # (B, N) - target tokens
        W_out: Optional[torch.Tensor] = None,     # (V, K) - output projection
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]]:
        """
        Forward pass - dispatches to appropriate FFN.

        Args:
            mu: Current beliefs (always required)
            beta: Attention weights (for variational/hamiltonian)
            mu_prior: Embedding priors (for variational/hamiltonian)
            phi: Gauge frames (for variational/hamiltonian)
            sigma: Covariances (for variational_full/hamiltonian)
            sigma_prior: Prior covariances (for hamiltonian - mass matrix)
            mask: Causal mask (for variational)
            targets: Target token IDs (for gradient_engine/hamiltonian E-step)
            W_out: Output projection matrix (for computing CE gradient)

        Returns:
            - 'learned': mu_out (B, N, K)
            - 'variational_*': mu_out (B, N, K)
            - 'variational_gradient_engine': (mu_out, sigma_out)
            - 'hamiltonian': (mu_out, sigma_out, phi_out, diagnostics)
        """
        if self.mode == 'learned':
            # Standard learned FFN
            return self.learned_ffn(mu)

        elif self.mode == 'variational_approx':
            # Check required inputs
            if beta is None or mu_prior is None or phi is None:
                raise ValueError("variational_approx requires beta, mu_prior, phi")

            return self.variational_ffn(
                mu=mu,
                beta=beta,
                mu_prior=mu_prior,
                phi=phi,
                mask=mask,
            )

        elif self.mode == 'variational_full':
            # Check required inputs
            if beta is None or mu_prior is None or phi is None:
                raise ValueError("variational_full requires beta, mu_prior, phi")

            return self.variational_ffn(
                mu=mu,
                beta=beta,
                mu_prior=mu_prior,
                phi=phi,
                sigma=sigma,
                mask=mask,
            )

        elif self.mode == 'variational_gradient_engine':
            # Check required inputs
            if beta is None or mu_prior is None or phi is None:
                raise ValueError("variational_gradient_engine requires beta, mu_prior, phi")

            # Gradient engine returns (mu, sigma) tuple
            # E-STEP: Minimize full F including DISCRETE observations (cross-entropy)
            mu_out, sigma_out = self.variational_ffn(
                mu=mu,
                beta=beta,
                mu_prior=mu_prior,
                phi=phi,
                sigma=sigma,
                mask=mask,
                targets=targets,  # Target tokens as DISCRETE observations!
                W_out=W_out,      # Output projection for computing ∂CE/∂μ
            )
            # Return BOTH mu and sigma (full Gaussian updates!)
            return (mu_out, sigma_out)

        elif self.mode == 'VFE_dynamic':
            # Dynamic-β VFE with attention-belief co-evolution
            # β is recomputed at EACH VFE step, enabling emergent block structure
            if mu_prior is None or phi is None:
                raise ValueError("VFE_dynamic requires mu_prior, phi")

            # Dynamic VFE returns (mu, sigma, beta_history)
            # beta_history is None unless return_beta_history=True is passed
            mu_out, sigma_out, beta_history = self.variational_ffn(
                mu=mu,
                beta=beta,          # Initial β (will be recomputed each step)
                mu_prior=mu_prior,
                phi=phi,
                sigma=sigma,
                mask=mask,
                targets=targets,
                W_out=W_out,
                return_beta_history=False,  # Set True for debugging/visualization
            )
            # Return (mu, sigma) like gradient_engine for compatibility
            return (mu_out, sigma_out)

        elif self.mode == 'VFE_dynamic_stable':
            # Stabilized dynamic-β VFE (RECOMMENDED!)
            # Same as VFE_dynamic but with temperature annealing + gradient balancing
            if mu_prior is None or phi is None:
                raise ValueError("VFE_dynamic_stable requires mu_prior, phi")

            mu_out, sigma_out, beta_history = self.variational_ffn(
                mu=mu,
                beta=beta,
                mu_prior=mu_prior,
                phi=phi,
                sigma=sigma,
                mask=mask,
                targets=targets,
                W_out=W_out,
                return_beta_history=False,
            )
            return (mu_out, sigma_out)

        elif self.mode == 'hamiltonian':
            # Check required inputs
            if mu_prior is None or phi is None or sigma is None:
                raise ValueError("hamiltonian mode requires mu_prior, phi, sigma")

            # Use prior covariance as mass matrix if not provided
            if sigma_prior is None:
                # Default: identity prior (unit mass)
                B, N, K = mu.shape
                sigma_prior = torch.eye(K, device=mu.device, dtype=mu.dtype).unsqueeze(0).unsqueeze(0).expand(B, N, -1, -1)

            # Hamiltonian dynamics returns (mu, sigma, phi, diagnostics)
            # Symplectic integration preserving energy!
            mu_out, sigma_out, phi_out, diagnostics = self.hamiltonian_ffn(
                mu=mu,
                Sigma=sigma,
                phi=phi,
                mu_prior=mu_prior,
                Sigma_prior=sigma_prior,
                beta=beta,          # Attention weights (optional)
                targets=targets,    # Target tokens for CE term
                W_out=W_out,        # Output projection
            )
            # Return full phase space update with diagnostics
            return (mu_out, sigma_out, phi_out, diagnostics)

        else:
            raise ValueError(f"Unknown mode: {self.mode}")

    def set_mode(self, mode: str):
        """
        Switch FFN mode at runtime.

        Mode aliases:
            - 'standard' -> 'learned' (baseline MLP)
            - 'VFE' -> 'variational_gradient_engine' (variational free energy)
        """
        # Normalize mode aliases
        if mode == 'standard':
            mode = 'learned'
        elif mode == 'VFE':
            mode = 'variational_gradient_engine'

        valid_modes = ['learned', 'variational_approx', 'variational_full', 'variational_gradient_engine', 'VFE_dynamic', 'VFE_dynamic_stable', 'hamiltonian']
        if mode not in valid_modes:
            raise ValueError(f"Invalid mode: {mode}. Valid modes: {valid_modes} (or aliases: 'standard', 'VFE')")

        if mode in ['variational_approx', 'variational_full', 'variational_gradient_engine', 'VFE_dynamic', 'VFE_dynamic_stable']:
            if not hasattr(self, 'variational_ffn'):
                raise ValueError(f"Mode {mode} not initialized")

        if mode == 'hamiltonian':
            if not hasattr(self, 'hamiltonian_ffn'):
                raise ValueError("Hamiltonian mode not initialized")

        self.mode = mode

    def get_mode(self) -> str:
        """Get current FFN mode."""
        return self.mode

    def get_hamiltonian_diagnostics(self) -> Optional[dict]:
        """Get diagnostics from last Hamiltonian forward pass."""
        if hasattr(self, 'hamiltonian_ffn') and hasattr(self.hamiltonian_ffn, 'last_diagnostics'):
            return self.hamiltonian_ffn.last_diagnostics
        return None


# =============================================================================
# Convenience functions
# =============================================================================

def create_ffn(
    embed_dim: int,
    hidden_dim: int,
    generators: Optional[torch.Tensor] = None,
    mode: str = 'learned',
    **kwargs
) -> GaugeFFN:
    """
    Factory function for creating FFN with correct mode.

    Example:
        >>> # Learned FFN (standard)
        >>> ffn = create_ffn(embed_dim=11, hidden_dim=44, mode='learned')

        >>> # Variational FFN (approximate)
        >>> ffn = create_ffn(
        ...     embed_dim=11, hidden_dim=44, mode='variational_approx',
        ...     generators=generators, alpha=0.001
        ... )
    """
    return GaugeFFN(
        embed_dim=embed_dim,
        hidden_dim=hidden_dim,
        generators=generators,
        mode=mode,
        **kwargs
    )


def convert_to_variational(
    ffn_module: GaugeFFN,
    mode: Literal['variational_approx', 'variational_full'],
    generators: torch.Tensor,
    **kwargs
) -> GaugeFFN:
    """
    Convert existing learned FFN to variational mode.

    Useful for:
    - Ablation studies
    - Progressive training (learned → variational)
    - Comparison experiments

    Args:
        ffn_module: Existing GaugeFFN module
        mode: Target variational mode
        generators: SO(3) generators
        **kwargs: Variational parameters

    Returns:
        Same module, now with variational mode initialized and active
    """
    # Initialize variational FFN
    if mode == 'variational_approx':
        ffn_module.variational_ffn = VariationalFFNApproximate(
            embed_dim=ffn_module.embed_dim,
            generators=generators,
            **kwargs
        )
    elif mode == 'variational_full':
        ffn_module.variational_ffn = VariationalFFNFull(
            embed_dim=ffn_module.embed_dim,
            generators=generators,
            **kwargs
        )
    else:
        raise ValueError(f"Invalid mode: {mode}")

    # Switch to variational mode
    ffn_module.set_mode(mode)

    return ffn_module