{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauge Frame Semantic Analysis\n",
    "\n",
    "Tests whether gauge frames φ encode semantic relationships.\n",
    "\n",
    "**Hypothesis**: Semantically related tokens have similar φ (gauge frames), so transport Ω_ij ≈ I between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - SET THESE PATHS\n",
    "EXPERIMENT_CONFIG_PATH = \"runs/your_experiment/experiment_config.json\"  # <-- CHANGE THIS\n",
    "CHECKPOINT_PATH = \"runs/your_experiment/best_model.pt\"                   # <-- CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    print(\"Loaded GPT-2 tokenizer\")\n",
    "except ImportError:\n",
    "    print(\"Install tiktoken: pip install tiktoken\")\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment config\n",
    "config_path = Path(EXPERIMENT_CONFIG_PATH)\n",
    "if config_path.exists():\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    print(f\"Loaded config: {config_path}\")\n",
    "    print(f\"  embed_dim: {config.get('embed_dim', 'N/A')}\")\n",
    "    print(f\"  lambda_beta: {config.get('lambda_beta', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"Config not found: {config_path}\")\n",
    "    config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint_path = Path(CHECKPOINT_PATH)\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    print(f\"Loaded checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Get state dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Find embeddings\n",
    "    mu_embed = None\n",
    "    phi_embed = None\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        if 'mu_embed' in key and 'weight' in key:\n",
    "            mu_embed = value\n",
    "            print(f\"  Found mu_embed: {value.shape}\")\n",
    "        if 'phi_embed' in key and 'weight' in key:\n",
    "            phi_embed = value\n",
    "            print(f\"  Found phi_embed: {value.shape}\")\n",
    "    \n",
    "    if mu_embed is None:\n",
    "        print(\"  WARNING: No mu_embed found!\")\n",
    "    if phi_embed is None:\n",
    "        print(\"  WARNING: No phi_embed found (model may use fixed gauge frames)\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    mu_embed = None\n",
    "    phi_embed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Semantic Distance Comparison\n",
    "\n",
    "Compare φ distances for related vs unrelated word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    \"\"\"Get mu and phi embeddings for a word.\"\"\"\n",
    "    if tokenizer is None:\n",
    "        return None, None\n",
    "    tokens = tokenizer.encode(word)\n",
    "    if not tokens:\n",
    "        return None, None\n",
    "    token_id = tokens[0]\n",
    "    \n",
    "    mu = mu_embed[token_id] if mu_embed is not None and token_id < len(mu_embed) else None\n",
    "    phi = phi_embed[token_id] if phi_embed is not None and token_id < len(phi_embed) else None\n",
    "    return mu, phi\n",
    "\n",
    "def distance(a, b):\n",
    "    \"\"\"Euclidean distance.\"\"\"\n",
    "    if a is None or b is None:\n",
    "        return float('nan')\n",
    "    return torch.norm(a - b).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# IMPORTANT: BPE tokens != full words!\n# Let's first explore what tokens we actually have\n\nprint(\"=\" * 60)\nprint(\"EXPLORING BPE VOCABULARY\")\nprint(\"=\" * 60)\n\n# Check which words are single tokens\ntest_words = [\"cat\", \"dog\", \"the\", \"and\", \"run\", \"big\", \"man\", \"day\", \n              \"kitten\", \"airplane\", \"democracy\", \"happy\", \"running\"]\n\nprint(\"\\nSingle-token words:\")\nsingle_tokens = {}\nfor word in test_words:\n    tokens = tokenizer.encode(word)\n    if len(tokens) == 1:\n        single_tokens[word] = tokens[0]\n        print(f\"  '{word}' -> token {tokens[0]}\")\n    else:\n        decoded = [tokenizer.decode([t]) for t in tokens]\n        print(f\"  '{word}' -> MULTI-TOKEN: {tokens} = {decoded}\")\n\n# For BPE, better to test token-level relationships:\n# - Single letters (a, b, c) vs digits (0, 1, 2) vs punctuation (!, ?, .)\n# - Common subwords that appear together in similar contexts\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TOKEN-LEVEL ANALYSIS (more reliable for BPE)\")\nprint(\"=\" * 60)\n\n# These are actual single tokens in GPT-2\n# Letters (lowercase single chars are tokens 64-89 approximately)\n# Digits are tokens 15-24\n# Punctuation varies\n\n# Get some example tokens\nletter_tokens = []\ndigit_tokens = []\npunct_tokens = []\n\nfor tid in range(256):  # First 256 are mostly single chars\n    try:\n        s = tokenizer.decode([tid])\n        if len(s) == 1:\n            if s.isalpha():\n                letter_tokens.append((tid, s))\n            elif s.isdigit():\n                digit_tokens.append((tid, s))\n            elif not s.isalnum() and not s.isspace():\n                punct_tokens.append((tid, s))\n    except:\n        pass\n\nprint(f\"Found {len(letter_tokens)} letter tokens, {len(digit_tokens)} digit tokens, {len(punct_tokens)} punct tokens\")"
  },
  {
   "cell_type": "code",
   "source": "# TOKEN-LEVEL DISTANCE ANALYSIS\n# Compare: letters vs letters, letters vs digits, letters vs punctuation\n\nprint(\"=\" * 60)\nprint(\"INTRA-CLASS vs INTER-CLASS DISTANCES\")\nprint(\"=\" * 60)\n\ndef get_embed_by_id(tid):\n    \"\"\"Get mu and phi for a token ID.\"\"\"\n    mu = mu_embed[tid] if mu_embed is not None and tid < len(mu_embed) else None\n    phi = phi_embed[tid] if phi_embed is not None and tid < len(phi_embed) else None\n    return mu, phi\n\n# Sample some tokens from each class\nimport random\nn_samples = min(10, len(letter_tokens), len(digit_tokens), len(punct_tokens))\nsample_letters = random.sample(letter_tokens, n_samples) if len(letter_tokens) >= n_samples else letter_tokens\nsample_digits = random.sample(digit_tokens, n_samples) if len(digit_tokens) >= n_samples else digit_tokens\nsample_punct = random.sample(punct_tokens, n_samples) if len(punct_tokens) >= n_samples else punct_tokens\n\n# Compute intra-class distances (letter-letter, digit-digit)\nintra_mu = []\nintra_phi = []\nfor i, (tid1, s1) in enumerate(sample_letters):\n    for tid2, s2 in sample_letters[i+1:]:\n        mu1, phi1 = get_embed_by_id(tid1)\n        mu2, phi2 = get_embed_by_id(tid2)\n        if mu1 is not None and mu2 is not None:\n            intra_mu.append(distance(mu1, mu2))\n        if phi1 is not None and phi2 is not None:\n            intra_phi.append(distance(phi1, phi2))\n\n# Compute inter-class distances (letter-digit, letter-punct)\ninter_mu = []\ninter_phi = []\nfor tid1, s1 in sample_letters:\n    for tid2, s2 in sample_digits + sample_punct:\n        mu1, phi1 = get_embed_by_id(tid1)\n        mu2, phi2 = get_embed_by_id(tid2)\n        if mu1 is not None and mu2 is not None:\n            inter_mu.append(distance(mu1, mu2))\n        if phi1 is not None and phi2 is not None:\n            inter_phi.append(distance(phi1, phi2))\n\nprint(f\"\\nmu embeddings:\")\nprint(f\"  Intra-class (letter-letter): {np.mean(intra_mu):.4f} +/- {np.std(intra_mu):.4f}\")\nprint(f\"  Inter-class (letter-digit/punct): {np.mean(inter_mu):.4f} +/- {np.std(inter_mu):.4f}\")\nif intra_mu and inter_mu:\n    print(f\"  Ratio (inter/intra): {np.mean(inter_mu)/np.mean(intra_mu):.2f}x\")\n\nif intra_phi and inter_phi:\n    print(f\"\\nphi embeddings (gauge frames):\")\n    print(f\"  Intra-class (letter-letter): {np.mean(intra_phi):.4f} +/- {np.std(intra_phi):.4f}\")\n    print(f\"  Inter-class (letter-digit/punct): {np.mean(inter_phi):.4f} +/- {np.std(inter_phi):.4f}\")\n    print(f\"  Ratio (inter/intra): {np.mean(inter_phi)/np.mean(intra_phi):.2f}x\")\n    \n    if np.mean(inter_phi) > np.mean(intra_phi) * 1.2:\n        print(f\"\\n  RESULT: phi shows class structure!\")\n    else:\n        print(f\"\\n  RESULT: phi does NOT show clear class structure.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances\n",
    "print(\"=\" * 60)\n",
    "print(\"RELATED PAIRS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "related_mu_dists = []\n",
    "related_phi_dists = []\n",
    "\n",
    "for w1, w2 in related_pairs:\n",
    "    mu1, phi1 = get_embeddings(w1)\n",
    "    mu2, phi2 = get_embeddings(w2)\n",
    "    \n",
    "    mu_d = distance(mu1, mu2)\n",
    "    phi_d = distance(phi1, phi2)\n",
    "    \n",
    "    if not np.isnan(mu_d): related_mu_dists.append(mu_d)\n",
    "    if not np.isnan(phi_d): related_phi_dists.append(phi_d)\n",
    "    \n",
    "    print(f\"{w1:12} - {w2:12}: mu_dist={mu_d:.4f}, phi_dist={phi_d:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"UNRELATED PAIRS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "unrelated_mu_dists = []\n",
    "unrelated_phi_dists = []\n",
    "\n",
    "for w1, w2 in unrelated_pairs:\n",
    "    mu1, phi1 = get_embeddings(w1)\n",
    "    mu2, phi2 = get_embeddings(w2)\n",
    "    \n",
    "    mu_d = distance(mu1, mu2)\n",
    "    phi_d = distance(phi1, phi2)\n",
    "    \n",
    "    if not np.isnan(mu_d): unrelated_mu_dists.append(mu_d)\n",
    "    if not np.isnan(phi_d): unrelated_phi_dists.append(phi_d)\n",
    "    \n",
    "    print(f\"{w1:12} - {w2:12}: mu_dist={mu_d:.4f}, phi_dist={phi_d:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if related_mu_dists and unrelated_mu_dists:\n",
    "    mu_ratio = np.mean(unrelated_mu_dists) / np.mean(related_mu_dists)\n",
    "    print(f\"\\nmu embeddings:\")\n",
    "    print(f\"  Related mean:   {np.mean(related_mu_dists):.4f}\")\n",
    "    print(f\"  Unrelated mean: {np.mean(unrelated_mu_dists):.4f}\")\n",
    "    print(f\"  Ratio:          {mu_ratio:.2f}x\")\n",
    "\n",
    "if related_phi_dists and unrelated_phi_dists:\n",
    "    phi_ratio = np.mean(unrelated_phi_dists) / np.mean(related_phi_dists)\n",
    "    print(f\"\\nphi embeddings (gauge frames):\")\n",
    "    print(f\"  Related mean:   {np.mean(related_phi_dists):.4f}\")\n",
    "    print(f\"  Unrelated mean: {np.mean(unrelated_phi_dists):.4f}\")\n",
    "    print(f\"  Ratio:          {phi_ratio:.2f}x\")\n",
    "    \n",
    "    if phi_ratio > 1.3:\n",
    "        print(f\"\\n  RESULT: phi DOES encode semantic relationships!\")\n",
    "    else:\n",
    "        print(f\"\\n  RESULT: phi does NOT clearly encode semantics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Clustering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if phi_embed is not None and tokenizer is not None:\n",
    "    # Categorize first 500 tokens\n",
    "    categories = {'letters': [], 'digits': [], 'punct': [], 'space': []}\n",
    "    \n",
    "    for tid in range(min(500, len(phi_embed))):\n",
    "        try:\n",
    "            s = tokenizer.decode([tid])\n",
    "            if s.strip().isalpha():\n",
    "                categories['letters'].append(tid)\n",
    "            elif s.strip().isdigit():\n",
    "                categories['digits'].append(tid)\n",
    "            elif s.strip() and not s.strip().isalnum():\n",
    "                categories['punct'].append(tid)\n",
    "            elif s.startswith(' '):\n",
    "                categories['space'].append(tid)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # PCA on phi\n",
    "    if phi_embed.shape[1] >= 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        phi_2d = pca.fit_transform(phi_embed[:500].numpy())\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = {'letters': 'blue', 'digits': 'red', 'punct': 'green', 'space': 'orange'}\n",
    "        \n",
    "        for cat, ids in categories.items():\n",
    "            if ids:\n",
    "                plt.scatter(phi_2d[ids, 0], phi_2d[ids, 1], \n",
    "                           c=colors[cat], label=f\"{cat} (n={len(ids)})\", alpha=0.6)\n",
    "        \n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(\"PCA of phi embeddings (gauge frames)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"phi_embed has < 2 dimensions, skipping PCA plot\")\n",
    "else:\n",
    "    print(\"No phi embeddings or tokenizer available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for mu embeddings\n",
    "if mu_embed is not None and tokenizer is not None:\n",
    "    pca = PCA(n_components=2)\n",
    "    mu_2d = pca.fit_transform(mu_embed[:500].numpy())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for cat, ids in categories.items():\n",
    "        if ids:\n",
    "            plt.scatter(mu_2d[ids, 0], mu_2d[ids, 1], \n",
    "                       c=colors[cat], label=f\"{cat} (n={len(ids)})\", alpha=0.6)\n",
    "    \n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(\"PCA of mu embeddings (belief means)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Transport Ablation\n",
    "\n",
    "What happens if we set all transport operators to identity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would require modifying the model and re-running inference.\n",
    "# To test, set use_identity_transport=True in attention config.\n",
    "print(\"To test transport ablation:\")\n",
    "print(\"1. Set use_identity_transport=True in model config\")\n",
    "print(\"2. Re-run evaluation on validation set\")\n",
    "print(\"3. Compare val PPL with vs without transport\")\n",
    "print(\"\")\n",
    "print(\"If val PPL is WORSE without transport -> transport carries information\")\n",
    "print(\"If val PPL is SAME without transport -> transport is not essential\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}